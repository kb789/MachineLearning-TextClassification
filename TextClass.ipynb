{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "import re, nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD,  NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to S3\n",
    "bucket='bbc-kb789'\n",
    "conn = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>wiki_name</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 1994 Mindoro earthquake occurred on Novemb...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1994_Mindoro_earthquake</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 1917 Bali earthquake occurred at 06:50 loc...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1917_Bali_earthquake</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 1941 Colima earthquake occurred on April 1...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1941_Colima_earthquake</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 1983 Coalinga earthquake occurred on May 2...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1983_Coalinga_earthquake</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2013 Bushehr earthquake occurred with a mo...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>2013_Bushehr_earthquake</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     l1            l2  \\\n",
       "0  The 1994 Mindoro earthquake occurred on Novemb...  Event  NaturalEvent   \n",
       "1  The 1917 Bali earthquake occurred at 06:50 loc...  Event  NaturalEvent   \n",
       "2  The 1941 Colima earthquake occurred on April 1...  Event  NaturalEvent   \n",
       "3  The 1983 Coalinga earthquake occurred on May 2...  Event  NaturalEvent   \n",
       "4  The 2013 Bushehr earthquake occurred with a mo...  Event  NaturalEvent   \n",
       "\n",
       "           l3                 wiki_name  word_count  \n",
       "0  Earthquake   1994_Mindoro_earthquake          59  \n",
       "1  Earthquake      1917_Bali_earthquake          68  \n",
       "2  Earthquake    1941_Colima_earthquake         194  \n",
       "3  Earthquake  1983_Coalinga_earthquake          98  \n",
       "4  Earthquake   2013_Bushehr_earthquake          61  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#access csv file with dataset on s3\n",
    "bucket='dbpedia-kb789'\n",
    "data_key = 'DBP_wiki_data.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "db_file=pd.read_csv(data_location)\n",
    "db_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not being used\n",
    "db_file=db_file.drop(['l1', 'l2', 'wiki_name', 'word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 1994 Mindoro earthquake occurred on Novemb...</td>\n",
       "      <td>Earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 1917 Bali earthquake occurred at 06:50 loc...</td>\n",
       "      <td>Earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 1941 Colima earthquake occurred on April 1...</td>\n",
       "      <td>Earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 1983 Coalinga earthquake occurred on May 2...</td>\n",
       "      <td>Earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2013 Bushehr earthquake occurred with a mo...</td>\n",
       "      <td>Earthquake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document    Category\n",
       "0  The 1994 Mindoro earthquake occurred on Novemb...  Earthquake\n",
       "1  The 1917 Bali earthquake occurred at 06:50 loc...  Earthquake\n",
       "2  The 1941 Colima earthquake occurred on April 1...  Earthquake\n",
       "3  The 1983 Coalinga earthquake occurred on May 2...  Earthquake\n",
       "4  The 2013 Bushehr earthquake occurred with a mo...  Earthquake"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns and display first 5 rows\n",
    "db_file.columns = ['Document', 'Category']\n",
    "db_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if there are any articles that don't have labels\n",
    "db_file['Category'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>342781</td>\n",
       "      <td>342781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>342781</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Lemon Souffle (22 February 1991–8 October 2001...</td>\n",
       "      <td>Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Document Category\n",
       "count                                              342781   342781\n",
       "unique                                             342781      219\n",
       "top     Lemon Souffle (22 February 1991–8 October 2001...   Planet\n",
       "freq                                                    1     2700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data exploration\n",
    "db_file.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiologicalDatabase        187\n",
       "CanadianFootballTeam      190\n",
       "BeachVolleyballPlayer     194\n",
       "AnimangaCharacter         203\n",
       "Cycad                     204\n",
       "                         ... \n",
       "FootballMatch            2700\n",
       "AcademicJournal          2700\n",
       "Manga                    2700\n",
       "GolfPlayer               2700\n",
       "Planet                   2700\n",
       "Name: Category, Length: 219, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore how many times each label is used. \n",
    "db_file['Category'].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data visualization: range of the number of times each label is used.\n",
    "db_file['Category'].value_counts(sort=True).plot.bar(xticks=[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643.295337256149\n"
     ]
    }
   ],
   "source": [
    "#get average length of articles\n",
    "result = pd.DataFrame([[]])\n",
    "result = db_file['Document'].apply(len).mean()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert unpreprocessed text to numeric format\n",
    "tfidf_i = TfidfVectorizer()\n",
    "df_tf_i =  tfidf_i.fit_transform(db_file['Document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert unpreprocessed text to numeric format\n",
    "cv_i= CountVectorizer()\n",
    "df_cv_i= cv_i.fit_transform(db_file['Document']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set initial x to documents\n",
    "init_x=df_tf_i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set inital y to labels\n",
    "init_y=db_file['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train, text values\n",
    "x_itrain, x_itest, y_itrain, y_itest = train_test_split(init_x, init_y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier as benchmark model. \n",
    "clf_init=RandomForestClassifier(max_depth=5)\n",
    "\n",
    "#Train the model using the training sets \n",
    "clf_init.fit(x_itrain,y_itrain)\n",
    "\n",
    "#test model with test set\n",
    "ipredicted_categories=clf_init.predict(x_itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.477781926811053"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test accuracy of benchmark model\n",
    "accuracy_score(y_itest, ipredicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    \"\"\"\n",
    "    inputs document\n",
    "    outputs text cleaned document\n",
    "    \"\"\"\n",
    "    #lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # tokenize document\n",
    "    word_tokens = word_tokenize(doc)\n",
    "    \n",
    "    # filter stopwords out of document\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stopwords: \n",
    "            filtered_sentence.append(w) \n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_sentence)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "#SOURCE OF CODE: https://towardsdatascience.com/text-preprocessing-with-nltk-9de5de891658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize document\n",
    "norm_doc = db_file['Document'].apply(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    mindoro earthquake occurred november local tim...\n",
       "1    bali earthquake occurred local time january ja...\n",
       "2    colima earthquake occurred april utc local tim...\n",
       "3    coalinga earthquake occurred may exactly utc c...\n",
       "4    bushehr earthquake occurred moment magnitude a...\n",
       "Name: Document, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display first 5 rows of normalized documents\n",
    "norm_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    \"\"\"\n",
    "    inputs nltk pos tag\n",
    "    outputs wordnet pos tag\n",
    "    \"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "#initialze lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#SOURCE OF CODE: https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    inputs sentence\n",
    "    outputs lemmatized sentence\n",
    "    \"\"\"\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#SOURCE OF CODE: https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize document\n",
    "lemm_doc= norm_doc.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    mindoro earthquake occur november local time n...\n",
       "1    bali earthquake occur local time january janua...\n",
       "2    colima earthquake occur april utc local time e...\n",
       "3    coalinga earthquake occur may exactly utc coal...\n",
       "4    bushehr earthquake occur moment magnitude apri...\n",
       "Name: Document, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check lemmatized document\n",
    "lemm_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert normalized, lemmatized text to numeric format\n",
    "cv_lemm = CountVectorizer()\n",
    "cv_lemm_df = cv_lemm.fit_transform(lemm_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert normalized, lemmatized text to numeric format\n",
    "tdif_lem = TfidfVectorizer()\n",
    "tdif_lem_df =  tdif_lem.fit_transform(lemm_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set x to normalized, lemmatized documents\n",
    "lem_x=tdif_lem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set y to categories\n",
    "lem_y=db_file['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split normalized, lemmatized dataset into train, text values\n",
    "x_ntrain, x_ntest, y_ntrain, y_ntest = train_test_split(lem_x, lem_y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier as a model\n",
    "clfnorm=RandomForestClassifier(max_depth=5)\n",
    "\n",
    "#fit the normalized,lemmatized training data to the model\n",
    "clfnorm.fit(x_ntrain, y_ntrain)\n",
    "\n",
    "#test the normalized,lemmatized test data on the model\n",
    "npredicted_categories = clfnorm.predict(x_ntest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4789255041075429"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find accuracy of model trained on normalized, lemmatized documents\n",
    "accuracy_score(y_ntest, npredicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=60, random_state=42)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit normalized, lemmatized document corpus to svd to get topics\n",
    "svd = TruncatedSVD(n_components=60, random_state=42)\n",
    "svd.fit(tdif_lem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform normalized, lemmatized document corpus to svd\n",
    "x_svd=svd.transform(tdif_lem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "season\n",
      " \n",
      "football\n",
      " \n",
      "play\n",
      " \n",
      "league\n",
      " \n",
      "team\n",
      " \n",
      "club\n",
      " \n",
      "championship\n",
      " \n",
      "Topic 1: \n",
      "football\n",
      " \n",
      "league\n",
      " \n",
      "season\n",
      " \n",
      "team\n",
      " \n",
      "play\n",
      " \n",
      "club\n",
      " \n",
      "coach\n",
      " \n",
      "Topic 2: \n",
      "album\n",
      " \n",
      "release\n",
      " \n",
      "single\n",
      " \n",
      "song\n",
      " \n",
      "band\n",
      " \n",
      "record\n",
      " \n",
      "championship\n",
      " \n",
      "Topic 3: \n",
      "album\n",
      " \n",
      "football\n",
      " \n",
      "release\n",
      " \n",
      "league\n",
      " \n",
      "season\n",
      " \n",
      "song\n",
      " \n",
      "record\n",
      " \n",
      "Topic 4: \n",
      "party\n",
      " \n",
      "election\n",
      " \n",
      "state\n",
      " \n",
      "university\n",
      " \n",
      "serve\n",
      " \n",
      "bear\n",
      " \n",
      "american\n",
      " \n",
      "Topic 5: \n",
      "race\n",
      " \n",
      "stake\n",
      " \n",
      "season\n",
      " \n",
      "horse\n",
      " \n",
      "thoroughbred\n",
      " \n",
      "run\n",
      " \n",
      "prix\n",
      " \n",
      "Topic 6: \n",
      "specie\n",
      " \n",
      "family\n",
      " \n",
      "habitat\n",
      " \n",
      "tropical\n",
      " \n",
      "subtropical\n",
      " \n",
      "genus\n",
      " \n",
      "moist\n",
      " \n",
      "Topic 7: \n",
      "coach\n",
      " \n",
      "university\n",
      " \n",
      "school\n",
      " \n",
      "college\n",
      " \n",
      "head\n",
      " \n",
      "basketball\n",
      " \n",
      "football\n",
      " \n",
      "Topic 8: \n",
      "play\n",
      " \n",
      "professional\n",
      " \n",
      "hockey\n",
      " \n",
      "tour\n",
      " \n",
      "station\n",
      " \n",
      "school\n",
      " \n",
      "player\n",
      " \n",
      "Topic 9: \n",
      "station\n",
      " \n",
      "football\n",
      " \n",
      "line\n",
      " \n",
      "party\n",
      " \n",
      "radio\n",
      " \n",
      "railway\n",
      " \n",
      "broadcast\n",
      " \n",
      "Topic 10: \n",
      "hockey\n",
      " \n",
      "ice\n",
      " \n",
      "station\n",
      " \n",
      "league\n",
      " \n",
      "team\n",
      " \n",
      "season\n",
      " \n",
      "party\n",
      " \n",
      "Topic 11: \n",
      "church\n",
      " \n",
      "diocese\n",
      " \n",
      "catholic\n",
      " \n",
      "hockey\n",
      " \n",
      "roman\n",
      " \n",
      "bishop\n",
      " \n",
      "league\n",
      " \n",
      "Topic 12: \n",
      "football\n",
      " \n",
      "race\n",
      " \n",
      "australian\n",
      " \n",
      "bear\n",
      " \n",
      "rugby\n",
      " \n",
      "footballer\n",
      " \n",
      "former\n",
      " \n",
      "Topic 13: \n",
      "rugby\n",
      " \n",
      "school\n",
      " \n",
      "union\n",
      " \n",
      "team\n",
      " \n",
      "airport\n",
      " \n",
      "club\n",
      " \n",
      "journal\n",
      " \n",
      "Topic 14: \n",
      "state\n",
      " \n",
      "united\n",
      " \n",
      "court\n",
      " \n",
      "league\n",
      " \n",
      "airport\n",
      " \n",
      "us\n",
      " \n",
      "hockey\n",
      " \n",
      "Topic 15: \n",
      "team\n",
      " \n",
      "station\n",
      " \n",
      "rugby\n",
      " \n",
      "church\n",
      " \n",
      "diocese\n",
      " \n",
      "school\n",
      " \n",
      "album\n",
      " \n",
      "Topic 16: \n",
      "bridge\n",
      " \n",
      "rugby\n",
      " \n",
      "state\n",
      " \n",
      "team\n",
      " \n",
      "union\n",
      " \n",
      "championship\n",
      " \n",
      "new\n",
      " \n",
      "Topic 17: \n",
      "school\n",
      " \n",
      "league\n",
      " \n",
      "olympics\n",
      " \n",
      "bridge\n",
      " \n",
      "high\n",
      " \n",
      "hockey\n",
      " \n",
      "event\n",
      " \n",
      "Topic 18: \n",
      "championship\n",
      " \n",
      "world\n",
      " \n",
      "school\n",
      " \n",
      "artistic\n",
      " \n",
      "habitat\n",
      " \n",
      "subtropical\n",
      " \n",
      "tropical\n",
      " \n",
      "Topic 19: \n",
      "habitat\n",
      " \n",
      "subtropical\n",
      " \n",
      "tropical\n",
      " \n",
      "moist\n",
      " \n",
      "olympics\n",
      " \n",
      "rugby\n",
      " \n",
      "forest\n",
      " \n",
      "Topic 20: \n",
      "airport\n",
      " \n",
      "airline\n",
      " \n",
      "hospital\n",
      " \n",
      "bridge\n",
      " \n",
      "service\n",
      " \n",
      "football\n",
      " \n",
      "party\n",
      " \n",
      "Topic 21: \n",
      "county\n",
      " \n",
      "game\n",
      " \n",
      "play\n",
      " \n",
      "medal\n",
      " \n",
      "senior\n",
      " \n",
      "allireland\n",
      " \n",
      "cricket\n",
      " \n",
      "Topic 22: \n",
      "school\n",
      " \n",
      "season\n",
      " \n",
      "rugby\n",
      " \n",
      "high\n",
      " \n",
      "bear\n",
      " \n",
      "union\n",
      " \n",
      "th\n",
      " \n",
      "Topic 23: \n",
      "dam\n",
      " \n",
      "cup\n",
      " \n",
      "airport\n",
      " \n",
      "school\n",
      " \n",
      "river\n",
      " \n",
      "air\n",
      " \n",
      "airline\n",
      " \n",
      "Topic 24: \n",
      "asteroid\n",
      " \n",
      "cup\n",
      " \n",
      "line\n",
      " \n",
      "mountain\n",
      " \n",
      "hospital\n",
      " \n",
      "th\n",
      " \n",
      "orbit\n",
      " \n",
      "Topic 25: \n",
      "asteroid\n",
      " \n",
      "orbit\n",
      " \n",
      "mountain\n",
      " \n",
      "school\n",
      " \n",
      "channel\n",
      " \n",
      "party\n",
      " \n",
      "range\n",
      " \n",
      "Topic 26: \n",
      "hospital\n",
      " \n",
      "asteroid\n",
      " \n",
      "dam\n",
      " \n",
      "orbit\n",
      " \n",
      "court\n",
      " \n",
      "league\n",
      " \n",
      "australian\n",
      " \n",
      "Topic 27: \n",
      "journal\n",
      " \n",
      "line\n",
      " \n",
      "railway\n",
      " \n",
      "publish\n",
      " \n",
      "club\n",
      " \n",
      "tour\n",
      " \n",
      "championship\n",
      " \n",
      "Topic 28: \n",
      "tour\n",
      " \n",
      "club\n",
      " \n",
      "golf\n",
      " \n",
      "th\n",
      " \n",
      "pga\n",
      " \n",
      "channel\n",
      " \n",
      "football\n",
      " \n",
      "Topic 29: \n",
      "new\n",
      " \n",
      "line\n",
      " \n",
      "dam\n",
      " \n",
      "asteroid\n",
      " \n",
      "york\n",
      " \n",
      "railway\n",
      " \n",
      "record\n",
      " \n",
      "Topic 30: \n",
      "journal\n",
      " \n",
      "airport\n",
      " \n",
      "bear\n",
      " \n",
      "county\n",
      " \n",
      "basketball\n",
      " \n",
      "miss\n",
      " \n",
      "league\n",
      " \n",
      "Topic 31: \n",
      "basketball\n",
      " \n",
      "league\n",
      " \n",
      "american\n",
      " \n",
      "comic\n",
      " \n",
      "division\n",
      " \n",
      "dam\n",
      " \n",
      "artist\n",
      " \n",
      "Topic 32: \n",
      "miss\n",
      " \n",
      "pageant\n",
      " \n",
      "beauty\n",
      " \n",
      "hospital\n",
      " \n",
      "model\n",
      " \n",
      "line\n",
      " \n",
      "universe\n",
      " \n",
      "Topic 33: \n",
      "basketball\n",
      " \n",
      "miss\n",
      " \n",
      "pageant\n",
      " \n",
      "division\n",
      " \n",
      "war\n",
      " \n",
      "beauty\n",
      " \n",
      "club\n",
      " \n",
      "Topic 34: \n",
      "museum\n",
      " \n",
      "mall\n",
      " \n",
      "new\n",
      " \n",
      "court\n",
      " \n",
      "tennis\n",
      " \n",
      "national\n",
      " \n",
      "church\n",
      " \n",
      "Topic 35: \n",
      "lake\n",
      " \n",
      "basketball\n",
      " \n",
      "club\n",
      " \n",
      "railway\n",
      " \n",
      "miss\n",
      " \n",
      "line\n",
      " \n",
      "bridge\n",
      " \n",
      "Topic 36: \n",
      "district\n",
      " \n",
      "county\n",
      " \n",
      "festival\n",
      " \n",
      "season\n",
      " \n",
      "film\n",
      " \n",
      "cricket\n",
      " \n",
      "village\n",
      " \n",
      "Topic 37: \n",
      "new\n",
      " \n",
      "team\n",
      " \n",
      "lake\n",
      " \n",
      "york\n",
      " \n",
      "de\n",
      " \n",
      "diocese\n",
      " \n",
      "formula\n",
      " \n",
      "Topic 38: \n",
      "mall\n",
      " \n",
      "museum\n",
      " \n",
      "state\n",
      " \n",
      "game\n",
      " \n",
      "highway\n",
      " \n",
      "rugby\n",
      " \n",
      "route\n",
      " \n",
      "Topic 39: \n",
      "church\n",
      " \n",
      "league\n",
      " \n",
      "baseball\n",
      " \n",
      "national\n",
      " \n",
      "football\n",
      " \n",
      "match\n",
      " \n",
      "team\n",
      " \n",
      "Topic 40: \n",
      "australian\n",
      " \n",
      "church\n",
      " \n",
      "basketball\n",
      " \n",
      "new\n",
      " \n",
      "cup\n",
      " \n",
      "team\n",
      " \n",
      "south\n",
      " \n",
      "Topic 41: \n",
      "tennis\n",
      " \n",
      "division\n",
      " \n",
      "district\n",
      " \n",
      "county\n",
      " \n",
      "new\n",
      " \n",
      "population\n",
      " \n",
      "tournament\n",
      " \n",
      "Topic 42: \n",
      "new\n",
      " \n",
      "film\n",
      " \n",
      "festival\n",
      " \n",
      "song\n",
      " \n",
      "church\n",
      " \n",
      "th\n",
      " \n",
      "york\n",
      " \n",
      "Topic 43: \n",
      "snail\n",
      " \n",
      "gastropod\n",
      " \n",
      "mollusk\n",
      " \n",
      "marine\n",
      " \n",
      "sea\n",
      " \n",
      "mall\n",
      " \n",
      "game\n",
      " \n",
      "Topic 44: \n",
      "record\n",
      " \n",
      "label\n",
      " \n",
      "snail\n",
      " \n",
      "australian\n",
      " \n",
      "festival\n",
      " \n",
      "coach\n",
      " \n",
      "league\n",
      " \n",
      "Topic 45: \n",
      "film\n",
      " \n",
      "festival\n",
      " \n",
      "handball\n",
      " \n",
      "player\n",
      " \n",
      "district\n",
      " \n",
      "national\n",
      " \n",
      "th\n",
      " \n",
      "Topic 46: \n",
      "cricket\n",
      " \n",
      "single\n",
      " \n",
      "season\n",
      " \n",
      "coach\n",
      " \n",
      "club\n",
      " \n",
      "route\n",
      " \n",
      "tennis\n",
      " \n",
      "Topic 47: \n",
      "museum\n",
      " \n",
      "state\n",
      " \n",
      "art\n",
      " \n",
      "season\n",
      " \n",
      "martial\n",
      " \n",
      "comic\n",
      " \n",
      "artist\n",
      " \n",
      "Topic 48: \n",
      "new\n",
      " \n",
      "basketball\n",
      " \n",
      "russian\n",
      " \n",
      "party\n",
      " \n",
      "play\n",
      " \n",
      "york\n",
      " \n",
      "launch\n",
      " \n",
      "Topic 49: \n",
      "mall\n",
      " \n",
      "film\n",
      " \n",
      "festival\n",
      " \n",
      "south\n",
      " \n",
      "album\n",
      " \n",
      "th\n",
      " \n",
      "journal\n",
      " \n",
      "Topic 50: \n",
      "comic\n",
      " \n",
      "song\n",
      " \n",
      "national\n",
      " \n",
      "russian\n",
      " \n",
      "bank\n",
      " \n",
      "champion\n",
      " \n",
      "league\n",
      " \n",
      "Topic 51: \n",
      "song\n",
      " \n",
      "mall\n",
      " \n",
      "de\n",
      " \n",
      "party\n",
      " \n",
      "division\n",
      " \n",
      "record\n",
      " \n",
      "contest\n",
      " \n",
      "Topic 52: \n",
      "bank\n",
      " \n",
      "comic\n",
      " \n",
      "american\n",
      " \n",
      "union\n",
      " \n",
      "book\n",
      " \n",
      "play\n",
      " \n",
      "company\n",
      " \n",
      "Topic 53: \n",
      "bank\n",
      " \n",
      "song\n",
      " \n",
      "mountain\n",
      " \n",
      "range\n",
      " \n",
      "record\n",
      " \n",
      "representative\n",
      " \n",
      "lake\n",
      " \n",
      "Topic 54: \n",
      "game\n",
      " \n",
      "stadium\n",
      " \n",
      "festival\n",
      " \n",
      "martial\n",
      " \n",
      "division\n",
      " \n",
      "mixed\n",
      " \n",
      "river\n",
      " \n",
      "Topic 55: \n",
      "river\n",
      " \n",
      "party\n",
      " \n",
      "museum\n",
      " \n",
      "court\n",
      " \n",
      "mall\n",
      " \n",
      "station\n",
      " \n",
      "cup\n",
      " \n",
      "Topic 56: \n",
      "museum\n",
      " \n",
      "satellite\n",
      " \n",
      "launch\n",
      " \n",
      "song\n",
      " \n",
      "record\n",
      " \n",
      "river\n",
      " \n",
      "university\n",
      " \n",
      "Topic 57: \n",
      "russian\n",
      " \n",
      "launch\n",
      " \n",
      "american\n",
      " \n",
      "club\n",
      " \n",
      "satellite\n",
      " \n",
      "stadium\n",
      " \n",
      "comic\n",
      " \n",
      "Topic 58: \n",
      "river\n",
      " \n",
      "club\n",
      " \n",
      "game\n",
      " \n",
      "tournament\n",
      " \n",
      "genus\n",
      " \n",
      "russian\n",
      " \n",
      "university\n",
      " \n",
      "Topic 59: \n",
      "handball\n",
      " \n",
      "de\n",
      " \n",
      "university\n",
      " \n",
      "airport\n",
      " \n",
      "division\n",
      " \n",
      "national\n",
      " \n",
      "channel\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#print topics found by svd\n",
    "terms = tdif_lem.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")\n",
    "        \n",
    "#SOURCE OF CODE: #https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, l1_ratio=0.5, n_components=60, random_state=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit normalized, lemmatized document corpus to nmf to get topics\n",
    "model = NMF(n_components=60, random_state=1,alpha=.1, l1_ratio=.5)\n",
    "model.fit(tdif_lem_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform normalized, lemmatized document corpus to nmf\n",
    "x_nmf=model.transform(tdif_lem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "team handball national womens uci member cycling\n",
      "\n",
      "Topic #1:\n",
      "new york zealand jersey city wale hampshire\n",
      "\n",
      "Topic #2:\n",
      "album release single song band chart studio\n",
      "\n",
      "Topic #3:\n",
      "championship world artistic gymnastics gymnast rhythmic nation\n",
      "\n",
      "Topic #4:\n",
      "mountain range peak glacier pas ft locate\n",
      "\n",
      "Topic #5:\n",
      "race stake horse thoroughbred run distance racehorse\n",
      "\n",
      "Topic #6:\n",
      "genus specie family spider find bromeliad cultivar\n",
      "\n",
      "Topic #7:\n",
      "party election seat vote political democratic liberal\n",
      "\n",
      "Topic #8:\n",
      "season finish th playoff division win first\n",
      "\n",
      "Topic #9:\n",
      "tour golf pga golfer professional play open\n",
      "\n",
      "Topic #10:\n",
      "hockey ice play junior nhl canadian ontario\n",
      "\n",
      "Topic #11:\n",
      "diocese catholic roman bishop latin archdiocese ecclesiastical\n",
      "\n",
      "Topic #12:\n",
      "football nfl play college draft club american\n",
      "\n",
      "Topic #13:\n",
      "first character year song also one time\n",
      "\n",
      "Topic #14:\n",
      "state court united supreme us case judge\n",
      "\n",
      "Topic #15:\n",
      "station radio broadcast fm format license own\n",
      "\n",
      "Topic #16:\n",
      "bridge span carry traffic cross road build\n",
      "\n",
      "Topic #17:\n",
      "school high student education public secondary district\n",
      "\n",
      "Topic #18:\n",
      "olympics summer compete medal freestyle swimmer metre\n",
      "\n",
      "Topic #19:\n",
      "habitat subtropical tropical moist forest frog natural\n",
      "\n",
      "Topic #20:\n",
      "airport airline air international service operate base\n",
      "\n",
      "Topic #21:\n",
      "senior allireland championship club hurl medal intercounty\n",
      "\n",
      "Topic #22:\n",
      "university college professor science student study research\n",
      "\n",
      "Topic #23:\n",
      "dam reservoir power water hydroelectric mw purpose\n",
      "\n",
      "Topic #24:\n",
      "cup final competition match winner round fa\n",
      "\n",
      "Topic #25:\n",
      "asteroid orbit observatory discover diameter astronomer planet\n",
      "\n",
      "Topic #26:\n",
      "hospital medical health care service bed center\n",
      "\n",
      "Topic #27:\n",
      "line railway station service operate rail train\n",
      "\n",
      "Topic #28:\n",
      "player bear double tennis professional rank badminton\n",
      "\n",
      "Topic #29:\n",
      "channel television tv network broadcast digital cable\n",
      "\n",
      "Topic #30:\n",
      "league baseball premier major division soccer professional\n",
      "\n",
      "Topic #31:\n",
      "journal publish peerreviewed magazine cover research academic\n",
      "\n",
      "Topic #32:\n",
      "miss pageant beauty model titleholder universe crown\n",
      "\n",
      "Topic #33:\n",
      "basketball play nba professional association player draft\n",
      "\n",
      "Topic #34:\n",
      "bank banking company commercial financial service branch\n",
      "\n",
      "Topic #35:\n",
      "lake water area km locate shore park\n",
      "\n",
      "Topic #36:\n",
      "race prix grand formula driver one motor\n",
      "\n",
      "Topic #37:\n",
      "church st parish england building anglican list\n",
      "\n",
      "Topic #38:\n",
      "mall shop store shopping locate anchor open\n",
      "\n",
      "Topic #39:\n",
      "australian footballer vfl rule victorian melbourne play\n",
      "\n",
      "Topic #40:\n",
      "series manga japanese anime magazine release japan\n",
      "\n",
      "Topic #41:\n",
      "tournament tennis edition atp challenger court place\n",
      "\n",
      "Topic #42:\n",
      "snail gastropod mollusk marine sea family specie\n",
      "\n",
      "Topic #43:\n",
      "film festival international short hold award animate\n",
      "\n",
      "Topic #44:\n",
      "event hold wrestling men match arena take\n",
      "\n",
      "Topic #45:\n",
      "county highway district route village km population\n",
      "\n",
      "Topic #46:\n",
      "game release video play develop stadium playstation\n",
      "\n",
      "Topic #47:\n",
      "museum art collection library history locate building\n",
      "\n",
      "Topic #48:\n",
      "rugby union club play trade rfc wale\n",
      "\n",
      "Topic #49:\n",
      "record label music band release independent found\n",
      "\n",
      "Topic #50:\n",
      "champion skater medalist figure russian skate junior\n",
      "\n",
      "Topic #51:\n",
      "war army force air th regiment command\n",
      "\n",
      "Topic #52:\n",
      "comic book artist american work writer marvel\n",
      "\n",
      "Topic #53:\n",
      "serve politician representative house american governor member\n",
      "\n",
      "Topic #54:\n",
      "martial mixed division artist fight ufc ultimate\n",
      "\n",
      "Topic #55:\n",
      "de french france la spanish chteau spain\n",
      "\n",
      "Topic #56:\n",
      "satellite launch earth eclipse solar kosmos occur\n",
      "\n",
      "Topic #57:\n",
      "river tributary flow km creek mile south\n",
      "\n",
      "Topic #58:\n",
      "coach head assistant university serve ncaa record\n",
      "\n",
      "Topic #59:\n",
      "cricket match firstclass play cricketer score make\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print topics found by nmf\n",
    "n_top_words = 7\n",
    "feature_names = tdif_lem.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "#SOURCE OF CODE: https://scikit-learn.org/0.15/auto_examples/applications/topics_extraction_with_nmf.html      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(max_iter=5, n_components=60, random_state=42)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit normalized, lemmatized document corpus to lda to get topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 60, random_state = 42, max_iter=5)\n",
    "lda.fit(cv_lemm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 words for Topic #0\n",
      "['khan', 'christian', 'greek', 'one', 'orthodox', 'know', 'saint']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #1\n",
      "['player', 'hockey', 'team', 'game', 'season', 'play', 'league']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #2\n",
      "['player', 'record', 'video', 'first', 'version', 'release', 'game']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #3\n",
      "['hill', 'line', 'seoul', 'korean', 'south', 'korea', 'station']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #4\n",
      "['spider', 'name', 'know', 'find', 'family', 'genus', 'specie']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #5\n",
      "['cross', 'license', 'radio', 'format', 'fm', 'earthquake', 'station']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #6\n",
      "['art', 'award', 'international', 'event', 'hold', 'film', 'festival']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #7\n",
      "['flow', 'power', 'reservoir', 'water', 'lake', 'dam', 'river']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #8\n",
      "['serve', 'carolina', 'congress', 'march', 'iowa', 'maine', 'south']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #9\n",
      "['mi', 'province', 'martial', 'population', 'also', 'district', 'village']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #10\n",
      "['law', 'judge', 'court', 'united', 'war', 'serve', 'state']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #11\n",
      "['music', 'chart', 'record', 'single', 'release', 'song', 'album']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #12\n",
      "['music', 'metal', 'swiss', 'band', 'railway', 'station', 'rock']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #13\n",
      "['unit', 'regiment', 'division', 'war', 'army', 'th', 'force']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #14\n",
      "['international', 'flight', 'operate', 'base', 'airport', 'airline', 'air']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #15\n",
      "['american', 'best', 'model', 'world', 'film', 'bear', 'miss']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #16\n",
      "['national', 'medal', 'compete', 'team', 'bear', 'championship', 'world']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #17\n",
      "['perform', 'variety', 'finnish', 'place', 'grape', 'contest', 'wine']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #18\n",
      "['politician', 'serve', 'president', 'elect', 'member', 'election', 'party']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #19\n",
      "['cup', 'player', 'league', 'club', 'team', 'rugby', 'play']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #20\n",
      "['cup', 'play', 'team', 'club', 'season', 'football', 'league']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #21\n",
      "['basketball', 'head', 'university', 'season', 'team', 'football', 'coach']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #22\n",
      "['make', 'south', 'first', 'australia', 'character', 'australian', 'play']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #23\n",
      "['english', 'sri', 'firstclass', 'make', 'match', 'cricket', 'county']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #24\n",
      "['diameter', 'observatory', 'planet', 'name', 'discover', 'orbit', 'asteroid']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #25\n",
      "['first', 'include', 'also', 'progress', 'state', 'new', 'game']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #26\n",
      "['shopping', 'open', 'centre', 'locate', 'shop', 'store', 'mall']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #27\n",
      "['news', 'issue', 'editor', 'publication', 'newspaper', 'publish', 'magazine']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #28\n",
      "['operate', 'company', 'station', 'service', 'bank', 'railway', 'line']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #29\n",
      "['championship', 'golf', 'play', 'professional', 'open', 'tournament', 'tour']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #30\n",
      "['also', 'habitat', 'small', 'family', 'find', 'bird', 'specie']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #31\n",
      "['grand', 'win', 'run', 'one', 'stake', 'horse', 'race']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #32\n",
      "['program', 'network', 'television', 'broadcast', 'radio', 'station', 'channel']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #33\n",
      "['service', 'library', 'center', 'stadium', 'locate', 'airport', 'hospital']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #34\n",
      "['science', 'journal', 'high', 'student', 'college', 'university', 'school']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #35\n",
      "['theatre', 'new', 'musical', 'include', 'label', 'music', 'record']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #36\n",
      "['kentucky', 'indiana', 'michigan', 'oklahoma', 'ohio', 'state', 'prison']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #37\n",
      "['build', 'building', 'cathedral', 'parish', 'italian', 'st', 'church']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #38\n",
      "['work', 'government', 'one', 'political', 'also', 'time', 'state']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #39\n",
      "['mile', 'united', 'state', 'creek', 'county', 'township', 'pennsylvania']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #40\n",
      "['archdiocese', 'latin', 'church', 'roman', 'bishop', 'catholic', 'diocese']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #41\n",
      "['german', 'fight', 'french', 'army', 'force', 'war', 'battle']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #42\n",
      "['new', 'island', 'build', 'river', 'route', 'road', 'bridge']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #43\n",
      "['medal', 'compete', 'race', 'team', 'summer', 'event', 'olympics']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #44\n",
      "['subtropical', 'find', 'tropical', 'forest', 'habitat', 'family', 'specie']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #45\n",
      "['form', 'record', 'rock', 'release', 'galaxy', 'album', 'band']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #46\n",
      "['pinyin', 'nrhplisted', 'town', 'city', 'china', 'firm', 'chinese']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #47\n",
      "['first', 'northern', 'county', 'dublin', 'bear', 'ireland', 'irish']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #48\n",
      "['player', 'series', 'event', 'armenian', 'world', 'ukrainian', 'poker']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #49\n",
      "['ny', 'zealand', 'state', 'trade', 'york', 'union', 'new']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #50\n",
      "['architect', 'house', 'music', 'work', 'collection', 'art', 'museum']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #51\n",
      "['release', 'anime', 'event', 'japan', 'manga', 'japanese', 'series']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #52\n",
      "['hebrew', 'israeli', 'ride', 'turkish', 'israel', 'roller', 'coaster']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #53\n",
      "['hold', 'supreme', 'case', 'us', 'united', 'court', 'state']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #54\n",
      "['north', 'km', 'state', 'locate', 'range', 'lake', 'mountain']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #55\n",
      "['spain', 'first', 'spanish', 'france', 'french', 'la', 'de']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #56\n",
      "['apple', 'century', 'chteau', 'th', 'brazilian', 'castle', 'de']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #57\n",
      "['father', 'death', 'daughter', 'ii', 'die', 'king', 'son']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #58\n",
      "['mollusk', 'gastropod', 'snail', 'specie', 'marine', 'sea', 'family']\n",
      "\n",
      "\n",
      "Top 7 words for Topic #59\n",
      "['orbit', 'cartoon', 'book', 'comic', 'satellite', 'launch', 'hotel']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print topics found by lda\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Top 7 words for Topic #{index}')\n",
    "    print([cv_lemm.get_feature_names()[i] for i in topic.argsort()[-7:]])\n",
    "    print('\\n')\n",
    "#SOURCE OF CODE: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform normalized, lemmatized document corpus to lda\n",
    "x_lda=lda.transform(cv_lemm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lda features to sparse matrix  \n",
    "sLDA = sparse.csr_matrix(x_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert nmf features to sparse matrix \n",
    "sNMF = sparse.csr_matrix(x_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert svd features to sparse matrix \n",
    "sSVD = sparse.csr_matrix(x_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacks the 4 sparse matrices horizontally into COO matrix\n",
    "x=hstack((tdif_lem_df, sLDA, sNMF, sSVD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts COO matrix to CSR matrix, which is X value to be loaded into classifier\n",
    "x=sparse.csr_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets y value equal to article labels\n",
    "y=db_file['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split normalized, lemmatized dataset with svd, nmf, and lda features and labels into train, text values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier with max depth of 5\n",
    "clf=RandomForestClassifier(max_depth=5)\n",
    "\n",
    "#Train the model using the training sets \n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "#Find the predicted labels for the test set\n",
    "y_pred=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.559267643764003"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print accuracy score of the predicted labels\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final Gaussian Classifier\n",
    "clf=RandomForestClassifier(max_depth=50, n_estimators=150)\n",
    "\n",
    "#Train the model using the training sets \n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "#Find the predicted labels for the test set\n",
    "y_pred=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8417662434652726"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print accuracy score of the predicted labels\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictArticle(text):\n",
    "    \"\"\"\n",
    "    inputs article not in train/test corpus\n",
    "    outputs predicted topic label of article\n",
    "    \"\"\"\n",
    "    #normalizes article\n",
    "    text=normalize_document(text)\n",
    "    #lemmatizes article\n",
    "    text=lemmatize_sentence(text)\n",
    "    #converts article to numeric data using count vectorizer\n",
    "    X = cv_lemm.transform([text])\n",
    "    #gets lda features of article\n",
    "    lda_text=lda.transform(X)\n",
    "    #converts article to numeric data using TF-IDF Vectorizer\n",
    "    X = tdif_lem.transform([text])\n",
    "    #gets nmf features of article\n",
    "    nmf_text=model.transform(X)\n",
    "    #gets svd features of article\n",
    "    svd_text=svd.transform(X)\n",
    "    #convert to csr matrices\n",
    "    sLDA_text = sparse.csr_matrix(lda_text)\n",
    "    sNMF_text = sparse.csr_matrix(nmf_text)\n",
    "    sSVD_text = sparse.csr_matrix(svd_text)\n",
    "    #converts article to numeric data using TF-IDF Vectorizer\n",
    "    x_text=tdif_lem.transform([text])\n",
    "    #Stacks the 4 sparse matrices horizontally into COO matrix\n",
    "    x_text=hstack((x_text, sLDA_text, sNMF_text, sSVD_text))\n",
    "    #Converts COO matrix to CSR matrix, which is X value to be loaded into classifier\n",
    "    text_matrix=sparse.csr_matrix(x_text)\n",
    "    #returns model's predicted label for article\n",
    "    return clf.predict(text_matrix)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Album']\n"
     ]
    }
   ],
   "source": [
    "text=['''\n",
    "The Beatles broke up five decades ago, but you would never know it from looking at the charts. In late September the group reissued the 50th anniversary edition of its 1969 album \"Abbey Road,\" which hit No. 3 on the Billboard 200 in the U.S. and the top spot in the U.K. The deluxe reissue features a remixed version of the original album and also includes two discs' worth of previously unheard material, a 5.1 surround-sound mix on Blu-ray, and a 100-page hardcover book. At a price just under $100, it's easy to see why someone would be tempted to buy it, even if they already own the original version of the album. The physical media sales are only part of the story. Billboard noted that to reach the No. 3 spot, the group had sold 81,000 physical albums, but according to Forbes, the group’s music has been streamed on Spotify 1.7 billion times in 2019. The group doing 30% of that streaming is between the ages of 18 and 24, followed by 25- to 29-year-olds, at 17%. That means almost half of the streaming is coming from people under the age of 30.\n",
    "''']\n",
    "text = ' '.join(text)\n",
    "print(predictArticle(text))\n",
    "#Source of article: https://www.cnbc.com/2019/10/26/the-beatles-remain-a-pop-culture-phenomenon-even-among-gen-z-fans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['River']\n"
     ]
    }
   ],
   "source": [
    "text2 = [\"\"\"\n",
    "Nile River, Arabic Baḥr Al-Nīl or Nahr Al-Nīl, the longest river in the world, called the father of African rivers. It rises south of the Equator and flows northward through northeastern Africa to drain into the Mediterranean Sea. It has a length of about 4,132 miles (6,650 kilometres) and drains an area estimated at 1,293,000 square miles (3,349,000 square kilometres). Its basin includes parts of Tanzania, Burundi, Rwanda, the Democratic Republic of the Congo, Kenya, Uganda, South Sudan, Ethiopia, Sudan, and the cultivated part of Egypt. Its most distant source is the Kagera River in Burundi. The Nile is formed by three principal streams: the Blue Nile (Arabic: Al-Baḥr Al-Azraq; Amharic: Abay) and the Atbara (Arabic: Nahr ʿAṭbarah), which flow from the highlands of Ethiopia, and the White Nile (Arabic: Al-Baḥr Al-Abyad), the headstreams of which flow into Lakes Victoria and Albert.\n",
    "\"\"\"]\n",
    "text2 = ' '.join(text2)\n",
    "print(predictArticle(text2))\n",
    "#Source of article: https://www.britannica.com/place/Nile-River"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GolfPlayer']\n"
     ]
    }
   ],
   "source": [
    "text3 = [\"\"\"\n",
    "Tiger (Eldrick) Woods, born December 30, 1975, is a professional golfer and entrepreneur. Since turning professional in 1996, Tiger has built an unprecedented competitive career. His achievements on the course–106 worldwide wins and 15 majors–have mirrored his success off the course as well. Woods serves as Founder and CEO of TGR, a multibrand enterprise comprised of his various companies and philanthropic endeavors, including TGR Design, the golf course design company; The TGR Foundation, a charitable foundation; TGR Live, an events production company; and The Woods Jupiter, an upscale sports restaurant. He has 82 PGA TOUR wins, tied with Sam Snead, holding the record for most wins in history. His majors victories include the five Masters Tournaments, four PGA Championships, three U.S. Open Championships, and three British Open Championships. With his second Masters victory in 2001, Tiger became the first golfer ever to hold all four professional major championships at the same time.\n",
    "\"\"\"]\n",
    "text3 = ' '.join(text3)\n",
    "print(predictArticle(text3))\n",
    "#Source of article: https://tigerwoods.com/biography/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Planet']\n"
     ]
    }
   ],
   "source": [
    "text4 = [\"\"\"\n",
    "Saturn is the sixth planet from the Sun and the second largest planet in our solar system. Adorned with a dazzling system of icy rings, Saturn is unique among the planets. It is not the only planet to have rings, but none are as spectacular or as complex as Saturn's. Like fellow gas giant Jupiter, Saturn is a massive ball made mostly of hydrogen and helium. Surrounded by more than 60 known moons, Saturn is home to some of the most fascinating landscapes in our solar system. From the jets of water that spray from Enceladus to the methane lakes on smoggy Titan, the Saturn system is a rich source of scientific discovery and still holds many mysteries. The farthest planet from Earth discovered by the unaided human eye, Saturn has been known since ancient times. The planet is named for the Roman god of agriculture and wealth, who was also the father of Jupiter. With a radius of 36,183.7 miles (58,232 kilometers), Saturn is 9 times wider than Earth. If Earth were the size of a nickel, Saturn would be about as big as a volleyball. From an average distance of 886 million miles (1.4 billion kilometers), Saturn is 9.5 astronomical units away from the Sun. One astronomical unit (abbreviated as AU), is the distance from the Sun to Earth. From this distance, it takes sunlight 80 minutes to travel from the Sun to Saturn. Saturn has the second-shortest day in the solar system. One day on Saturn takes only 10.7 hours (the time it takes for Saturn to rotate or spin around once), and Saturn makes a complete orbit around the Sun (a year in Saturnian time) in about 29.4 Earth years (10,756 Earth days). Its axis is tilted by 26.73 degrees with respect to its orbit around the Sun, which is similar to Earth's 23.5-degree tilt. This means that, like Earth, Saturn experiences seasons. Like Jupiter, Saturn is made mostly of hydrogen and helium. At Saturn's center is a dense core of metals like iron and nickel surrounded by rocky material and other compounds solidified by the intense pressure and heat. It is enveloped by liquid metallic hydrogen inside a layer of liquid hydrogen—similar to Jupiter's core but considerably smaller. It's hard to imagine, but Saturn is the only planet in our solar system whose average density is less than water. The giant gas planet could float in a bathtub if such a colossal thing existed. Saturn took shape when the rest of the solar system formed about 4.5 billion years ago, when gravity pulled swirling gas and dust in to become this gas giant. About 4 billion years ago, Saturn settled into its current position in the outer solar system, where it is the sixth planet from the Sun. Like Jupiter, Saturn is mostly made of hydrogen and helium, the same two main components that make up the Sun.\n",
    "\"\"\"]\n",
    "text4 = ' '.join(text4)\n",
    "print(predictArticle(text4))\n",
    "#Source of article: https://solarsystem.nasa.gov/planets/saturn/in-depth/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
